{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The below approach is extremely slow, so ignore it.\n",
    "def _matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    '''\n",
    "    R: rating matrix\n",
    "    P: |U| * K (User features matrix)\n",
    "    Q: |D| * K (Item features matrix)\n",
    "    K: latent features\n",
    "    steps: iterations\n",
    "    alpha: learning rate\n",
    "    beta: regularization parameter'''\n",
    "    Q = Q.T\n",
    "\n",
    "    for step in tqdm(range(steps)):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    # calculate error\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "\n",
    "                    for k in range(K):\n",
    "                        # calculate gradient with a and beta parameter\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        eR = np.dot(P,Q)\n",
    "\n",
    "        e = 0\n",
    "\n",
    "        for i in range(len(R)):\n",
    "\n",
    "            for j in range(len(R[i])):\n",
    "\n",
    "                if R[i][j] > 0:\n",
    "\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "\n",
    "                    for k in range(K):\n",
    "\n",
    "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
    "        # 0.001: local minimum\n",
    "        if e < 0.001:\n",
    "\n",
    "            break\n",
    "\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_batches(R, batch_size):\n",
    "    \"\"\"Generate mini-batches for the non-zero entries in R.\"\"\"\n",
    "    non_zero_indices = cp.asarray(R).nonzero()\n",
    "    num_samples = len(non_zero_indices[0])\n",
    "    indices = cp.arange(num_samples)\n",
    "    cp.random.shuffle(indices)\n",
    "    \n",
    "    for idx in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[idx:idx+batch_size]\n",
    "        i_indices = non_zero_indices[0][batch_indices]\n",
    "        j_indices = non_zero_indices[1][batch_indices]\n",
    "        values = R[i_indices, j_indices]\n",
    "        yield i_indices, j_indices, values\n",
    "\n",
    "def matrix_factorization_gpu_batched(R, P, Q, K, batch_size=1000, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    R = cp.array(R)\n",
    "    P = cp.array(P)\n",
    "    Q = cp.array(Q)\n",
    "    Q = Q.T\n",
    "\n",
    "    for step in tqdm(range(steps), desc=\"Epochs\"):\n",
    "        for i_indices, j_indices, values in tqdm(get_batches(R, batch_size), desc=\"Rows\", leave=False):\n",
    "            errors = values - cp.sum(P[i_indices] * Q[:, j_indices].T, axis=1)\n",
    "            \n",
    "            for k in range(K):\n",
    "                P[i_indices, k] += alpha * (2 * errors * Q[k, j_indices] - beta * P[i_indices, k])\n",
    "                Q[k, j_indices] += alpha * (2 * errors * P[i_indices, k] - beta * Q[k, j_indices])\n",
    "\n",
    "        eR = cp.dot(P, Q)\n",
    "        e = cp.sum((R[R > 0] - eR[R > 0]) ** 2) + beta/2.0 * (cp.sum(P**2) + cp.sum(Q**2))\n",
    "        \n",
    "        if e < 0.001:\n",
    "            break\n",
    "\n",
    "    return cp.asnumpy(P), cp.asnumpy(Q.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATA_DIR = 'data'\n",
    "train_ratings = pd.read_csv(f'{DATA_DIR}/train_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 8983\n",
      "Number of unique users: 610\n"
     ]
    }
   ],
   "source": [
    "#how many unique movies and users do we have?\n",
    "n_movies = train_ratings.movieId.unique().shape[0]\n",
    "n_users = train_ratings.userId.unique().shape[0]\n",
    "print(f'Number of unique movies: {n_movies}')\n",
    "print(f'Number of unique users: {n_users}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = train_ratings.pivot_table(index='userId', columns='movieId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 8983)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(train_matrix)\n",
    "M = len(train_matrix.columns)\n",
    "N, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5000/5000 [06:43<00:00, 12.38it/s]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "nP, nQ = matrix_factorization_gpu_batched(train_matrix.values, P, Q, K, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((610, 10), (8983, 10))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nP.shape, nQ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test set\n",
    "test_set = pd.read_csv(f'{DATA_DIR}/test_set_no_ratings.csv')\n",
    "\n",
    "# Placeholder list to store the predicted ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "# Iterate over each row in the test set\n",
    "for _, row in test_set.iterrows():\n",
    "    userId = row['userId']\n",
    "    movieId = row['movieId']\n",
    "    \n",
    "    \n",
    "    # Check if userId and movieId exist in ratings_matrix\n",
    "    if userId in train_matrix.index and movieId in train_matrix.columns:\n",
    "        user_idx = list(train_matrix.index).index(userId)\n",
    "        movie_idx = list(train_matrix.columns).index(movieId)\n",
    "        \n",
    "        predicted_rating = np.dot(P[user_idx, :], Q[movie_idx, :].T)\n",
    "    else:\n",
    "        # Use a default value for unknown userIds or movieIds\n",
    "        predicted_rating = np.mean(train_matrix.values[train_matrix.values > 0])\n",
    "    \n",
    "    predicted_ratings.append(predicted_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predicted ratings to the test set dataframe and save to CSV\n",
    "test_set['rating'] = predicted_ratings\n",
    "predictions_df = test_set[['Id', 'rating']]\n",
    "predictions_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
