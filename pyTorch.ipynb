{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test = pd.read_csv('data/test_set_no_ratings.csv')\n",
    "# 1. Load Data\n",
    "train = pd.read_csv('data/train_ratings.csv')\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "\n",
    "# 2. Pre-processing: Convert genres into a binary encoded vector\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(movies['genres'])\n",
    "genres_df = pd.DataFrame(genres_encoded, columns=mlb.classes_, index=movies.movieId)\n",
    "\n",
    "# Merge genres data with main data\n",
    "train = train.merge(genres_df, left_on='movieId', right_index=True)\n",
    "test = test.merge(genres_df, left_on='movieId', right_index=True)\n",
    "\n",
    "\n",
    "# 3. Encode users and movies as integer indices\n",
    "user_enc = LabelEncoder()\n",
    "train['user'] = user_enc.fit_transform(train['userId'])\n",
    "test['user'] = user_enc.transform(test['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_enc = LabelEncoder()\n",
    "all_movies = movies['movieId'].unique().tolist()\n",
    "movie_enc.fit(all_movies)\n",
    "train['movie'] = movie_enc.transform(train['movieId'])\n",
    "test['movie'] = movie_enc.transform(test['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>(no genres listed)</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>...</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>77866</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "      <td>7333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>285</td>\n",
       "      <td>474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>539</td>\n",
       "      <td>599</td>\n",
       "      <td>474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>1789</td>\n",
       "      <td>447</td>\n",
       "      <td>474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>446</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>20147</td>\n",
       "      <td>509</td>\n",
       "      <td>103042</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>8183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20153</th>\n",
       "      <td>20153</td>\n",
       "      <td>212</td>\n",
       "      <td>140715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>20158</td>\n",
       "      <td>522</td>\n",
       "      <td>27006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>521</td>\n",
       "      <td>5608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160</th>\n",
       "      <td>20160</td>\n",
       "      <td>599</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20161</td>\n",
       "      <td>596</td>\n",
       "      <td>181719</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>9663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20168 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  userId  movieId  (no genres listed)  Action  Adventure  \\\n",
       "0          0     432    77866                   0       1          1   \n",
       "1          1     288      474                   0       1          0   \n",
       "196      196     285      474                   0       1          0   \n",
       "539      539     599      474                   0       1          0   \n",
       "1789    1789     447      474                   0       1          0   \n",
       "...      ...     ...      ...                 ...     ...        ...   \n",
       "20147  20147     509   103042                   0       1          1   \n",
       "20153  20153     212   140715                   0       0          0   \n",
       "20158  20158     522    27006                   0       0          0   \n",
       "20160  20160     599      229                   0       0          0   \n",
       "20161  20161     596   181719                   1       0          0   \n",
       "\n",
       "       Animation  Children  Comedy  Crime  ...  IMAX  Musical  Mystery  \\\n",
       "0              0         0       0      0  ...     0        0        0   \n",
       "1              0         0       0      0  ...     0        0        0   \n",
       "196            0         0       0      0  ...     0        0        0   \n",
       "539            0         0       0      0  ...     0        0        0   \n",
       "1789           0         0       0      0  ...     0        0        0   \n",
       "...          ...       ...     ...    ...  ...   ...      ...      ...   \n",
       "20147          0         0       0      0  ...     1        0        0   \n",
       "20153          0         0       0      0  ...     0        0        0   \n",
       "20158          0         0       0      0  ...     0        0        0   \n",
       "20160          0         0       0      0  ...     0        0        0   \n",
       "20161          0         0       0      0  ...     0        0        0   \n",
       "\n",
       "       Romance  Sci-Fi  Thriller  War  Western  user  movie  \n",
       "0            1       0         0    1        0   431   7333  \n",
       "1            0       0         1    0        0   287    412  \n",
       "196          0       0         1    0        0   284    412  \n",
       "539          0       0         1    0        0   598    412  \n",
       "1789         0       0         1    0        0   446    412  \n",
       "...        ...     ...       ...  ...      ...   ...    ...  \n",
       "20147        0       1         0    0        0   508   8183  \n",
       "20153        0       0         0    0        0   211   9024  \n",
       "20158        0       0         0    0        0   521   5608  \n",
       "20160        0       0         1    0        0   598    195  \n",
       "20161        0       0         0    0        0   595   9663  \n",
       "\n",
       "[20168 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the data\n",
    "X = train[['user', 'movie'] + mlb.classes_.tolist()]\n",
    "y = train['rating']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.long).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.long).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# 5. Build the Model\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, num_genres, emb_size):\n",
    "        super(Recommender, self).__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, emb_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, emb_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_size*2 + num_genres, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_input = x[:, 0]\n",
    "        movie_input = x[:, 1]\n",
    "        genres_input = x[:, 2:].float()\n",
    "        \n",
    "        user_emb = self.user_embedding(user_input)\n",
    "        movie_emb = self.movie_embedding(movie_input)\n",
    "        \n",
    "        concat = torch.cat([user_emb, movie_emb, genres_input], dim=1)\n",
    "        out = self.fc(concat)\n",
    "        \n",
    "        return out.squeeze()\n",
    "\n",
    "num_users = len(user_enc.classes_)\n",
    "num_movies = len(movie_enc.classes_)\n",
    "\n",
    "# 6. Train the Model\n",
    "model = Recommender(num_users, num_movies, len(mlb.classes_), 15).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/750, Training Loss: 12.87882137298584, Validation Loss: 12.758206367492676\n",
      "Epoch 2/750, Training Loss: 12.573097229003906, Validation Loss: 12.459717750549316\n",
      "Epoch 3/750, Training Loss: 12.257015228271484, Validation Loss: 12.148744583129883\n",
      "Epoch 4/750, Training Loss: 11.937586784362793, Validation Loss: 11.82282829284668\n",
      "Epoch 5/750, Training Loss: 11.60042667388916, Validation Loss: 11.480487823486328\n",
      "Epoch 6/750, Training Loss: 11.255151748657227, Validation Loss: 11.120407104492188\n",
      "Epoch 7/750, Training Loss: 10.897964477539062, Validation Loss: 10.742064476013184\n",
      "Epoch 8/750, Training Loss: 10.526307106018066, Validation Loss: 10.345647811889648\n",
      "Epoch 9/750, Training Loss: 10.13363265991211, Validation Loss: 9.931975364685059\n",
      "Epoch 10/750, Training Loss: 9.742566108703613, Validation Loss: 9.50211238861084\n",
      "Epoch 11/750, Training Loss: 9.338736534118652, Validation Loss: 9.057463645935059\n",
      "Epoch 12/750, Training Loss: 8.907567977905273, Validation Loss: 8.599844932556152\n",
      "Epoch 13/750, Training Loss: 8.47721004486084, Validation Loss: 8.13138198852539\n",
      "Epoch 14/750, Training Loss: 8.032334327697754, Validation Loss: 7.65427827835083\n",
      "Epoch 15/750, Training Loss: 7.573431968688965, Validation Loss: 7.171234607696533\n",
      "Epoch 16/750, Training Loss: 7.10786247253418, Validation Loss: 6.685122013092041\n",
      "Epoch 17/750, Training Loss: 6.645567417144775, Validation Loss: 6.199124336242676\n",
      "Epoch 18/750, Training Loss: 6.191190242767334, Validation Loss: 5.71653413772583\n",
      "Epoch 19/750, Training Loss: 5.729611396789551, Validation Loss: 5.240965843200684\n",
      "Epoch 20/750, Training Loss: 5.271011829376221, Validation Loss: 4.77619743347168\n",
      "Epoch 21/750, Training Loss: 4.824331760406494, Validation Loss: 4.32636022567749\n",
      "Epoch 22/750, Training Loss: 4.390933513641357, Validation Loss: 3.8957629203796387\n",
      "Epoch 23/750, Training Loss: 3.9857311248779297, Validation Loss: 3.4888408184051514\n",
      "Epoch 24/750, Training Loss: 3.588007688522339, Validation Loss: 3.110137939453125\n",
      "Epoch 25/750, Training Loss: 3.2248575687408447, Validation Loss: 2.764098882675171\n",
      "Epoch 26/750, Training Loss: 2.900846242904663, Validation Loss: 2.454942226409912\n",
      "Epoch 27/750, Training Loss: 2.61702036857605, Validation Loss: 2.1862974166870117\n",
      "Epoch 28/750, Training Loss: 2.3665378093719482, Validation Loss: 1.9609878063201904\n",
      "Epoch 29/750, Training Loss: 2.152322769165039, Validation Loss: 1.7805237770080566\n",
      "Epoch 30/750, Training Loss: 1.9979382753372192, Validation Loss: 1.6447697877883911\n",
      "Epoch 31/750, Training Loss: 1.8792438507080078, Validation Loss: 1.551576018333435\n",
      "Epoch 32/750, Training Loss: 1.8077661991119385, Validation Loss: 1.4966497421264648\n",
      "Epoch 33/750, Training Loss: 1.7804185152053833, Validation Loss: 1.4736796617507935\n",
      "Epoch 34/750, Training Loss: 1.7716999053955078, Validation Loss: 1.474644422531128\n",
      "Epoch 35/750, Training Loss: 1.7917569875717163, Validation Loss: 1.4905577898025513\n",
      "Epoch 36/750, Training Loss: 1.8196841478347778, Validation Loss: 1.5125136375427246\n",
      "Epoch 37/750, Training Loss: 1.8574129343032837, Validation Loss: 1.5325872898101807\n",
      "Epoch 38/750, Training Loss: 1.8894484043121338, Validation Loss: 1.5446579456329346\n",
      "Epoch 39/750, Training Loss: 1.9057778120040894, Validation Loss: 1.5450540781021118\n",
      "Epoch 40/750, Training Loss: 1.9046710729599, Validation Loss: 1.5326203107833862\n",
      "Epoch 41/750, Training Loss: 1.912079930305481, Validation Loss: 1.5079936981201172\n",
      "Epoch 42/750, Training Loss: 1.8693082332611084, Validation Loss: 1.4737563133239746\n",
      "Epoch 43/750, Training Loss: 1.830990195274353, Validation Loss: 1.4331800937652588\n",
      "Epoch 44/750, Training Loss: 1.797410488128662, Validation Loss: 1.3898638486862183\n",
      "Epoch 45/750, Training Loss: 1.7358503341674805, Validation Loss: 1.3473668098449707\n",
      "Epoch 46/750, Training Loss: 1.696288824081421, Validation Loss: 1.3085752725601196\n",
      "Epoch 47/750, Training Loss: 1.6324509382247925, Validation Loss: 1.2756143808364868\n",
      "Epoch 48/750, Training Loss: 1.5878968238830566, Validation Loss: 1.2497347593307495\n",
      "Epoch 49/750, Training Loss: 1.5582095384597778, Validation Loss: 1.2313271760940552\n",
      "Epoch 50/750, Training Loss: 1.533166527748108, Validation Loss: 1.2201039791107178\n",
      "Epoch 51/750, Training Loss: 1.508618712425232, Validation Loss: 1.2152460813522339\n",
      "Epoch 52/750, Training Loss: 1.5050908327102661, Validation Loss: 1.2155735492706299\n",
      "Epoch 53/750, Training Loss: 1.4897851943969727, Validation Loss: 1.2196853160858154\n",
      "Epoch 54/750, Training Loss: 1.4801909923553467, Validation Loss: 1.2261569499969482\n",
      "Epoch 55/750, Training Loss: 1.4839116334915161, Validation Loss: 1.2336406707763672\n",
      "Epoch 56/750, Training Loss: 1.4875907897949219, Validation Loss: 1.2409480810165405\n",
      "Epoch 57/750, Training Loss: 1.480204463005066, Validation Loss: 1.247147798538208\n",
      "Epoch 58/750, Training Loss: 1.4983102083206177, Validation Loss: 1.2515814304351807\n",
      "Epoch 59/750, Training Loss: 1.499112606048584, Validation Loss: 1.253780484199524\n",
      "Epoch 60/750, Training Loss: 1.4928467273712158, Validation Loss: 1.2535368204116821\n",
      "Epoch 61/750, Training Loss: 1.4881328344345093, Validation Loss: 1.250901699066162\n",
      "Epoch 62/750, Training Loss: 1.4811445474624634, Validation Loss: 1.2460776567459106\n",
      "Epoch 63/750, Training Loss: 1.4827996492385864, Validation Loss: 1.239345908164978\n",
      "Epoch 64/750, Training Loss: 1.4856351613998413, Validation Loss: 1.2311577796936035\n",
      "Epoch 65/750, Training Loss: 1.4649244546890259, Validation Loss: 1.2219388484954834\n",
      "Epoch 66/750, Training Loss: 1.454514503479004, Validation Loss: 1.212162971496582\n",
      "Epoch 67/750, Training Loss: 1.4524831771850586, Validation Loss: 1.2022712230682373\n",
      "Epoch 68/750, Training Loss: 1.4463049173355103, Validation Loss: 1.19260573387146\n",
      "Epoch 69/750, Training Loss: 1.441214680671692, Validation Loss: 1.1835060119628906\n",
      "Epoch 70/750, Training Loss: 1.4337553977966309, Validation Loss: 1.1752054691314697\n",
      "Epoch 71/750, Training Loss: 1.4194340705871582, Validation Loss: 1.1678096055984497\n",
      "Epoch 72/750, Training Loss: 1.4202134609222412, Validation Loss: 1.161330223083496\n",
      "Epoch 73/750, Training Loss: 1.415169358253479, Validation Loss: 1.1557594537734985\n",
      "Epoch 74/750, Training Loss: 1.4159409999847412, Validation Loss: 1.151001214981079\n",
      "Epoch 75/750, Training Loss: 1.4083483219146729, Validation Loss: 1.1469275951385498\n",
      "Epoch 76/750, Training Loss: 1.4090992212295532, Validation Loss: 1.1434028148651123\n",
      "Epoch 77/750, Training Loss: 1.4080336093902588, Validation Loss: 1.1402827501296997\n",
      "Epoch 78/750, Training Loss: 1.4017131328582764, Validation Loss: 1.137445092201233\n",
      "Epoch 79/750, Training Loss: 1.4063371419906616, Validation Loss: 1.1347945928573608\n",
      "Epoch 80/750, Training Loss: 1.3940521478652954, Validation Loss: 1.1322733163833618\n",
      "Epoch 81/750, Training Loss: 1.3947253227233887, Validation Loss: 1.1298449039459229\n",
      "Epoch 82/750, Training Loss: 1.3792825937271118, Validation Loss: 1.1275044679641724\n",
      "Epoch 83/750, Training Loss: 1.3814252614974976, Validation Loss: 1.1252754926681519\n",
      "Epoch 84/750, Training Loss: 1.3868987560272217, Validation Loss: 1.123181700706482\n",
      "Epoch 85/750, Training Loss: 1.3732545375823975, Validation Loss: 1.1212478876113892\n",
      "Epoch 86/750, Training Loss: 1.37916100025177, Validation Loss: 1.1194961071014404\n",
      "Epoch 87/750, Training Loss: 1.3738172054290771, Validation Loss: 1.1179192066192627\n",
      "Epoch 88/750, Training Loss: 1.3614801168441772, Validation Loss: 1.1165140867233276\n",
      "Epoch 89/750, Training Loss: 1.36324143409729, Validation Loss: 1.1152616739273071\n",
      "Epoch 90/750, Training Loss: 1.3713592290878296, Validation Loss: 1.1141446828842163\n",
      "Epoch 91/750, Training Loss: 1.3593252897262573, Validation Loss: 1.1130993366241455\n",
      "Epoch 92/750, Training Loss: 1.3574210405349731, Validation Loss: 1.1120661497116089\n",
      "Epoch 93/750, Training Loss: 1.3561947345733643, Validation Loss: 1.1109954118728638\n",
      "Epoch 94/750, Training Loss: 1.3549436330795288, Validation Loss: 1.109848976135254\n",
      "Epoch 95/750, Training Loss: 1.3462212085723877, Validation Loss: 1.108583688735962\n",
      "Epoch 96/750, Training Loss: 1.336729645729065, Validation Loss: 1.1071372032165527\n",
      "Epoch 97/750, Training Loss: 1.3456470966339111, Validation Loss: 1.1055002212524414\n",
      "Epoch 98/750, Training Loss: 1.3434988260269165, Validation Loss: 1.1036885976791382\n",
      "Epoch 99/750, Training Loss: 1.3389637470245361, Validation Loss: 1.1017292737960815\n",
      "Epoch 100/750, Training Loss: 1.337639331817627, Validation Loss: 1.099633812904358\n",
      "Epoch 101/750, Training Loss: 1.3335660696029663, Validation Loss: 1.097456693649292\n",
      "Epoch 102/750, Training Loss: 1.3230581283569336, Validation Loss: 1.0951858758926392\n",
      "Epoch 103/750, Training Loss: 1.3251745700836182, Validation Loss: 1.0928313732147217\n",
      "Epoch 104/750, Training Loss: 1.3289166688919067, Validation Loss: 1.0904721021652222\n",
      "Epoch 105/750, Training Loss: 1.3321030139923096, Validation Loss: 1.0881348848342896\n",
      "Epoch 106/750, Training Loss: 1.3220505714416504, Validation Loss: 1.0858114957809448\n",
      "Epoch 107/750, Training Loss: 1.3144567012786865, Validation Loss: 1.0835325717926025\n",
      "Epoch 108/750, Training Loss: 1.3114471435546875, Validation Loss: 1.0813014507293701\n",
      "Epoch 109/750, Training Loss: 1.3151358366012573, Validation Loss: 1.079148292541504\n",
      "Epoch 110/750, Training Loss: 1.3118295669555664, Validation Loss: 1.0770732164382935\n",
      "Epoch 111/750, Training Loss: 1.310998558998108, Validation Loss: 1.0750937461853027\n",
      "Epoch 112/750, Training Loss: 1.2996547222137451, Validation Loss: 1.0731886625289917\n",
      "Epoch 113/750, Training Loss: 1.299113392829895, Validation Loss: 1.0713516473770142\n",
      "Epoch 114/750, Training Loss: 1.3020011186599731, Validation Loss: 1.069571614265442\n",
      "Epoch 115/750, Training Loss: 1.2991113662719727, Validation Loss: 1.067874550819397\n",
      "Epoch 116/750, Training Loss: 1.2908827066421509, Validation Loss: 1.0662122964859009\n",
      "Epoch 117/750, Training Loss: 1.2884613275527954, Validation Loss: 1.064603567123413\n",
      "Epoch 118/750, Training Loss: 1.2963452339172363, Validation Loss: 1.0630570650100708\n",
      "Epoch 119/750, Training Loss: 1.2895143032073975, Validation Loss: 1.0615664720535278\n",
      "Epoch 120/750, Training Loss: 1.2856248617172241, Validation Loss: 1.0601199865341187\n",
      "Epoch 121/750, Training Loss: 1.2870876789093018, Validation Loss: 1.0587106943130493\n",
      "Epoch 122/750, Training Loss: 1.283190131187439, Validation Loss: 1.057328462600708\n",
      "Epoch 123/750, Training Loss: 1.280656337738037, Validation Loss: 1.0559595823287964\n",
      "Epoch 124/750, Training Loss: 1.2806535959243774, Validation Loss: 1.0546125173568726\n",
      "Epoch 125/750, Training Loss: 1.2837084531784058, Validation Loss: 1.053270936012268\n",
      "Epoch 126/750, Training Loss: 1.2799558639526367, Validation Loss: 1.0519261360168457\n",
      "Epoch 127/750, Training Loss: 1.2743417024612427, Validation Loss: 1.0505870580673218\n",
      "Epoch 128/750, Training Loss: 1.2724928855895996, Validation Loss: 1.0492326021194458\n",
      "Epoch 129/750, Training Loss: 1.2785500288009644, Validation Loss: 1.0478709936141968\n",
      "Epoch 130/750, Training Loss: 1.2738553285598755, Validation Loss: 1.0464905500411987\n",
      "Epoch 131/750, Training Loss: 1.2609565258026123, Validation Loss: 1.0451058149337769\n",
      "Epoch 132/750, Training Loss: 1.258974313735962, Validation Loss: 1.0436919927597046\n",
      "Epoch 133/750, Training Loss: 1.2614822387695312, Validation Loss: 1.042250394821167\n",
      "Epoch 134/750, Training Loss: 1.2592629194259644, Validation Loss: 1.0408061742782593\n",
      "Epoch 135/750, Training Loss: 1.2521843910217285, Validation Loss: 1.0393236875534058\n",
      "Epoch 136/750, Training Loss: 1.2506277561187744, Validation Loss: 1.037818193435669\n",
      "Epoch 137/750, Training Loss: 1.2548147439956665, Validation Loss: 1.0363168716430664\n",
      "Epoch 138/750, Training Loss: 1.2488588094711304, Validation Loss: 1.034805417060852\n",
      "Epoch 139/750, Training Loss: 1.2456562519073486, Validation Loss: 1.0332993268966675\n",
      "Epoch 140/750, Training Loss: 1.2483607530593872, Validation Loss: 1.0318175554275513\n",
      "Epoch 141/750, Training Loss: 1.2417538166046143, Validation Loss: 1.0303434133529663\n",
      "Epoch 142/750, Training Loss: 1.2491992712020874, Validation Loss: 1.0288946628570557\n",
      "Epoch 143/750, Training Loss: 1.2396763563156128, Validation Loss: 1.0274717807769775\n",
      "Epoch 144/750, Training Loss: 1.2490588426589966, Validation Loss: 1.026078224182129\n",
      "Epoch 145/750, Training Loss: 1.2381489276885986, Validation Loss: 1.024709939956665\n",
      "Epoch 146/750, Training Loss: 1.2349121570587158, Validation Loss: 1.023366093635559\n",
      "Epoch 147/750, Training Loss: 1.2311040163040161, Validation Loss: 1.0220423936843872\n",
      "Epoch 148/750, Training Loss: 1.2230702638626099, Validation Loss: 1.0207395553588867\n",
      "Epoch 149/750, Training Loss: 1.2292234897613525, Validation Loss: 1.0194402933120728\n",
      "Epoch 150/750, Training Loss: 1.2253623008728027, Validation Loss: 1.0181410312652588\n",
      "Epoch 151/750, Training Loss: 1.2243419885635376, Validation Loss: 1.0168673992156982\n",
      "Epoch 152/750, Training Loss: 1.218794345855713, Validation Loss: 1.0156102180480957\n",
      "Epoch 153/750, Training Loss: 1.2166163921356201, Validation Loss: 1.0143663883209229\n",
      "Epoch 154/750, Training Loss: 1.2212481498718262, Validation Loss: 1.0131655931472778\n",
      "Epoch 155/750, Training Loss: 1.2224563360214233, Validation Loss: 1.0119752883911133\n",
      "Epoch 156/750, Training Loss: 1.2128307819366455, Validation Loss: 1.0107717514038086\n",
      "Epoch 157/750, Training Loss: 1.2161822319030762, Validation Loss: 1.0096044540405273\n",
      "Epoch 158/750, Training Loss: 1.2107127904891968, Validation Loss: 1.008444905281067\n",
      "Epoch 159/750, Training Loss: 1.2091290950775146, Validation Loss: 1.0072778463363647\n",
      "Epoch 160/750, Training Loss: 1.2052741050720215, Validation Loss: 1.0060875415802002\n",
      "Epoch 161/750, Training Loss: 1.1994129419326782, Validation Loss: 1.0048718452453613\n",
      "Epoch 162/750, Training Loss: 1.207053303718567, Validation Loss: 1.0036684274673462\n",
      "Epoch 163/750, Training Loss: 1.2036768198013306, Validation Loss: 1.0024762153625488\n",
      "Epoch 164/750, Training Loss: 1.1993207931518555, Validation Loss: 1.0013182163238525\n",
      "Epoch 165/750, Training Loss: 1.2039352655410767, Validation Loss: 1.00015389919281\n",
      "Epoch 166/750, Training Loss: 1.1974037885665894, Validation Loss: 0.9990097284317017\n",
      "Epoch 167/750, Training Loss: 1.2026605606079102, Validation Loss: 0.9978662133216858\n",
      "Epoch 168/750, Training Loss: 1.1949400901794434, Validation Loss: 0.9967206716537476\n",
      "Epoch 169/750, Training Loss: 1.1951090097427368, Validation Loss: 0.9955748319625854\n",
      "Epoch 170/750, Training Loss: 1.1847163438796997, Validation Loss: 0.9944269061088562\n",
      "Epoch 171/750, Training Loss: 1.1879096031188965, Validation Loss: 0.9933128952980042\n",
      "Epoch 172/750, Training Loss: 1.1921203136444092, Validation Loss: 0.992230236530304\n",
      "Epoch 173/750, Training Loss: 1.1892529726028442, Validation Loss: 0.9911682605743408\n",
      "Epoch 174/750, Training Loss: 1.1856392621994019, Validation Loss: 0.9900898337364197\n",
      "Epoch 175/750, Training Loss: 1.17972731590271, Validation Loss: 0.9890173673629761\n",
      "Epoch 176/750, Training Loss: 1.1798194646835327, Validation Loss: 0.9879463911056519\n",
      "Epoch 177/750, Training Loss: 1.1806621551513672, Validation Loss: 0.9868682622909546\n",
      "Epoch 178/750, Training Loss: 1.1745381355285645, Validation Loss: 0.9857951998710632\n",
      "Epoch 179/750, Training Loss: 1.1771180629730225, Validation Loss: 0.9847210049629211\n",
      "Epoch 180/750, Training Loss: 1.1804838180541992, Validation Loss: 0.9836350679397583\n",
      "Epoch 181/750, Training Loss: 1.173579454421997, Validation Loss: 0.9825683236122131\n",
      "Epoch 182/750, Training Loss: 1.1779768466949463, Validation Loss: 0.9815154671669006\n",
      "Epoch 183/750, Training Loss: 1.1634801626205444, Validation Loss: 0.9804585576057434\n",
      "Epoch 184/750, Training Loss: 1.1684950590133667, Validation Loss: 0.979422390460968\n",
      "Epoch 185/750, Training Loss: 1.1645690202713013, Validation Loss: 0.9783654808998108\n",
      "Epoch 186/750, Training Loss: 1.1696207523345947, Validation Loss: 0.9773092269897461\n",
      "Epoch 187/750, Training Loss: 1.161132574081421, Validation Loss: 0.9762437343597412\n",
      "Epoch 188/750, Training Loss: 1.1692891120910645, Validation Loss: 0.9751636385917664\n",
      "Epoch 189/750, Training Loss: 1.1600737571716309, Validation Loss: 0.9741081595420837\n",
      "Epoch 190/750, Training Loss: 1.1499227285385132, Validation Loss: 0.9730754494667053\n",
      "Epoch 191/750, Training Loss: 1.1644095182418823, Validation Loss: 0.9720520377159119\n",
      "Epoch 192/750, Training Loss: 1.1584932804107666, Validation Loss: 0.9710354804992676\n",
      "Epoch 193/750, Training Loss: 1.1548405885696411, Validation Loss: 0.9700381755828857\n",
      "Epoch 194/750, Training Loss: 1.1592518091201782, Validation Loss: 0.9690566062927246\n",
      "Epoch 195/750, Training Loss: 1.1556522846221924, Validation Loss: 0.9680806398391724\n",
      "Epoch 196/750, Training Loss: 1.1560248136520386, Validation Loss: 0.9671198129653931\n",
      "Epoch 197/750, Training Loss: 1.1564891338348389, Validation Loss: 0.9661321043968201\n",
      "Epoch 198/750, Training Loss: 1.1443121433258057, Validation Loss: 0.9651556015014648\n",
      "Epoch 199/750, Training Loss: 1.1516631841659546, Validation Loss: 0.9641940593719482\n",
      "Epoch 200/750, Training Loss: 1.1452220678329468, Validation Loss: 0.9632291197776794\n",
      "Epoch 201/750, Training Loss: 1.1490988731384277, Validation Loss: 0.9622480273246765\n",
      "Epoch 202/750, Training Loss: 1.1430583000183105, Validation Loss: 0.9612639546394348\n",
      "Epoch 203/750, Training Loss: 1.1462419033050537, Validation Loss: 0.9602857828140259\n",
      "Epoch 204/750, Training Loss: 1.1395204067230225, Validation Loss: 0.9593161344528198\n",
      "Epoch 205/750, Training Loss: 1.1355987787246704, Validation Loss: 0.9583495855331421\n",
      "Epoch 206/750, Training Loss: 1.1372926235198975, Validation Loss: 0.9573847055435181\n",
      "Epoch 207/750, Training Loss: 1.1392179727554321, Validation Loss: 0.9564417004585266\n",
      "Epoch 208/750, Training Loss: 1.1358669996261597, Validation Loss: 0.9555040001869202\n",
      "Epoch 209/750, Training Loss: 1.1300817728042603, Validation Loss: 0.9545696377754211\n",
      "Epoch 210/750, Training Loss: 1.133072853088379, Validation Loss: 0.9536635279655457\n",
      "Epoch 211/750, Training Loss: 1.1296988725662231, Validation Loss: 0.9527727961540222\n",
      "Epoch 212/750, Training Loss: 1.1252108812332153, Validation Loss: 0.9518976807594299\n",
      "Epoch 213/750, Training Loss: 1.1291049718856812, Validation Loss: 0.9510378241539001\n",
      "Epoch 214/750, Training Loss: 1.122219204902649, Validation Loss: 0.9502043128013611\n",
      "Epoch 215/750, Training Loss: 1.1240047216415405, Validation Loss: 0.9493424296379089\n",
      "Epoch 216/750, Training Loss: 1.116697907447815, Validation Loss: 0.9484702348709106\n",
      "Epoch 217/750, Training Loss: 1.1273494958877563, Validation Loss: 0.9475890398025513\n",
      "Epoch 218/750, Training Loss: 1.1265888214111328, Validation Loss: 0.946743369102478\n",
      "Epoch 219/750, Training Loss: 1.1249457597732544, Validation Loss: 0.9459086656570435\n",
      "Epoch 220/750, Training Loss: 1.1209334135055542, Validation Loss: 0.9450612664222717\n",
      "Epoch 221/750, Training Loss: 1.1169465780258179, Validation Loss: 0.944198489189148\n",
      "Epoch 222/750, Training Loss: 1.1193608045578003, Validation Loss: 0.9433436393737793\n",
      "Epoch 223/750, Training Loss: 1.118630290031433, Validation Loss: 0.9424618482589722\n",
      "Epoch 224/750, Training Loss: 1.1152373552322388, Validation Loss: 0.9415934681892395\n",
      "Epoch 225/750, Training Loss: 1.1220899820327759, Validation Loss: 0.9407384991645813\n",
      "Epoch 226/750, Training Loss: 1.1165275573730469, Validation Loss: 0.9398974180221558\n",
      "Epoch 227/750, Training Loss: 1.1180373430252075, Validation Loss: 0.9391050338745117\n",
      "Epoch 228/750, Training Loss: 1.110230803489685, Validation Loss: 0.9383121132850647\n",
      "Epoch 229/750, Training Loss: 1.1138778924942017, Validation Loss: 0.9375059008598328\n",
      "Epoch 230/750, Training Loss: 1.1049597263336182, Validation Loss: 0.9367153644561768\n",
      "Epoch 231/750, Training Loss: 1.112260341644287, Validation Loss: 0.9359568357467651\n",
      "Epoch 232/750, Training Loss: 1.1089868545532227, Validation Loss: 0.9351999759674072\n",
      "Epoch 233/750, Training Loss: 1.1010794639587402, Validation Loss: 0.9344277381896973\n",
      "Epoch 234/750, Training Loss: 1.1022965908050537, Validation Loss: 0.933652400970459\n",
      "Epoch 235/750, Training Loss: 1.101990818977356, Validation Loss: 0.9328861236572266\n",
      "Epoch 236/750, Training Loss: 1.1057499647140503, Validation Loss: 0.9321111440658569\n",
      "Epoch 237/750, Training Loss: 1.102871060371399, Validation Loss: 0.9313414692878723\n",
      "Epoch 238/750, Training Loss: 1.1003918647766113, Validation Loss: 0.930572509765625\n",
      "Epoch 239/750, Training Loss: 1.0980931520462036, Validation Loss: 0.9297868609428406\n",
      "Epoch 240/750, Training Loss: 1.1002651453018188, Validation Loss: 0.9289863705635071\n",
      "Epoch 241/750, Training Loss: 1.0933310985565186, Validation Loss: 0.9281853437423706\n",
      "Epoch 242/750, Training Loss: 1.0924301147460938, Validation Loss: 0.927389919757843\n",
      "Epoch 243/750, Training Loss: 1.0898915529251099, Validation Loss: 0.9266207218170166\n",
      "Epoch 244/750, Training Loss: 1.0921497344970703, Validation Loss: 0.9258678555488586\n",
      "Epoch 245/750, Training Loss: 1.0885666608810425, Validation Loss: 0.9251006245613098\n",
      "Epoch 246/750, Training Loss: 1.0840444564819336, Validation Loss: 0.9243808388710022\n",
      "Epoch 247/750, Training Loss: 1.0780171155929565, Validation Loss: 0.9236589670181274\n",
      "Epoch 248/750, Training Loss: 1.0876399278640747, Validation Loss: 0.9229466915130615\n",
      "Epoch 249/750, Training Loss: 1.0886980295181274, Validation Loss: 0.9222310185432434\n",
      "Epoch 250/750, Training Loss: 1.085152268409729, Validation Loss: 0.9215131998062134\n",
      "Epoch 251/750, Training Loss: 1.0841187238693237, Validation Loss: 0.9208173155784607\n",
      "Epoch 252/750, Training Loss: 1.0797864198684692, Validation Loss: 0.9201266169548035\n",
      "Epoch 253/750, Training Loss: 1.0878241062164307, Validation Loss: 0.9194380044937134\n",
      "Epoch 254/750, Training Loss: 1.0825308561325073, Validation Loss: 0.9187518954277039\n",
      "Epoch 255/750, Training Loss: 1.0791352987289429, Validation Loss: 0.9180561304092407\n",
      "Epoch 256/750, Training Loss: 1.0765142440795898, Validation Loss: 0.9173666834831238\n",
      "Epoch 257/750, Training Loss: 1.079148530960083, Validation Loss: 0.9166645407676697\n",
      "Epoch 258/750, Training Loss: 1.0825409889221191, Validation Loss: 0.9159407019615173\n",
      "Epoch 259/750, Training Loss: 1.074601173400879, Validation Loss: 0.9151979684829712\n",
      "Epoch 260/750, Training Loss: 1.0841665267944336, Validation Loss: 0.9144790172576904\n",
      "Epoch 261/750, Training Loss: 1.0756961107254028, Validation Loss: 0.9137775897979736\n",
      "Epoch 262/750, Training Loss: 1.0741208791732788, Validation Loss: 0.9130609035491943\n",
      "Epoch 263/750, Training Loss: 1.0720001459121704, Validation Loss: 0.9123349785804749\n",
      "Epoch 264/750, Training Loss: 1.0731384754180908, Validation Loss: 0.9116344451904297\n",
      "Epoch 265/750, Training Loss: 1.0700243711471558, Validation Loss: 0.9109624028205872\n",
      "Epoch 266/750, Training Loss: 1.0653647184371948, Validation Loss: 0.9103167653083801\n",
      "Epoch 267/750, Training Loss: 1.0754179954528809, Validation Loss: 0.909680962562561\n",
      "Epoch 268/750, Training Loss: 1.069511890411377, Validation Loss: 0.9090381264686584\n",
      "Epoch 269/750, Training Loss: 1.060806393623352, Validation Loss: 0.9083915948867798\n",
      "Epoch 270/750, Training Loss: 1.0598536729812622, Validation Loss: 0.9077452421188354\n",
      "Epoch 271/750, Training Loss: 1.0713731050491333, Validation Loss: 0.9071415066719055\n",
      "Epoch 272/750, Training Loss: 1.0597810745239258, Validation Loss: 0.9065521955490112\n",
      "Epoch 273/750, Training Loss: 1.062839388847351, Validation Loss: 0.9059549570083618\n",
      "Epoch 274/750, Training Loss: 1.0653812885284424, Validation Loss: 0.9053536057472229\n",
      "Epoch 275/750, Training Loss: 1.0664840936660767, Validation Loss: 0.9047445058822632\n",
      "Epoch 276/750, Training Loss: 1.0590754747390747, Validation Loss: 0.9041399359703064\n",
      "Epoch 277/750, Training Loss: 1.0595667362213135, Validation Loss: 0.9035328030586243\n",
      "Epoch 278/750, Training Loss: 1.0581319332122803, Validation Loss: 0.9029257297515869\n",
      "Epoch 279/750, Training Loss: 1.0606223344802856, Validation Loss: 0.9022936224937439\n",
      "Epoch 280/750, Training Loss: 1.0587923526763916, Validation Loss: 0.9016087651252747\n",
      "Epoch 281/750, Training Loss: 1.0514695644378662, Validation Loss: 0.9009491801261902\n",
      "Epoch 282/750, Training Loss: 1.0585960149765015, Validation Loss: 0.9002857208251953\n",
      "Epoch 283/750, Training Loss: 1.0556668043136597, Validation Loss: 0.8996581435203552\n",
      "Epoch 284/750, Training Loss: 1.0511666536331177, Validation Loss: 0.8990490436553955\n",
      "Epoch 285/750, Training Loss: 1.0480211973190308, Validation Loss: 0.8984344005584717\n",
      "Epoch 286/750, Training Loss: 1.0583078861236572, Validation Loss: 0.8978447914123535\n",
      "Epoch 287/750, Training Loss: 1.049540400505066, Validation Loss: 0.8972358703613281\n",
      "Epoch 288/750, Training Loss: 1.05150306224823, Validation Loss: 0.8966195583343506\n",
      "Epoch 289/750, Training Loss: 1.0425326824188232, Validation Loss: 0.8959600329399109\n",
      "Epoch 290/750, Training Loss: 1.0408190488815308, Validation Loss: 0.8952891826629639\n",
      "Epoch 291/750, Training Loss: 1.0450680255889893, Validation Loss: 0.8946242928504944\n",
      "Epoch 292/750, Training Loss: 1.0491477251052856, Validation Loss: 0.8940016627311707\n",
      "Epoch 293/750, Training Loss: 1.0428546667099, Validation Loss: 0.8933960199356079\n",
      "Epoch 294/750, Training Loss: 1.0476030111312866, Validation Loss: 0.8928308486938477\n",
      "Epoch 295/750, Training Loss: 1.042741060256958, Validation Loss: 0.8922996520996094\n",
      "Epoch 296/750, Training Loss: 1.0433263778686523, Validation Loss: 0.891773521900177\n",
      "Epoch 297/750, Training Loss: 1.042280912399292, Validation Loss: 0.8912829160690308\n",
      "Epoch 298/750, Training Loss: 1.0364490747451782, Validation Loss: 0.8908078670501709\n",
      "Epoch 299/750, Training Loss: 1.0315672159194946, Validation Loss: 0.8903290033340454\n",
      "Epoch 300/750, Training Loss: 1.0389251708984375, Validation Loss: 0.8898586630821228\n",
      "Epoch 301/750, Training Loss: 1.0413808822631836, Validation Loss: 0.8893782496452332\n",
      "Epoch 302/750, Training Loss: 1.0380353927612305, Validation Loss: 0.888893187046051\n",
      "Epoch 303/750, Training Loss: 1.0340551137924194, Validation Loss: 0.8884100317955017\n",
      "Epoch 304/750, Training Loss: 1.0401406288146973, Validation Loss: 0.8879448771476746\n",
      "Epoch 305/750, Training Loss: 1.0319935083389282, Validation Loss: 0.8874565362930298\n",
      "Epoch 306/750, Training Loss: 1.0374804735183716, Validation Loss: 0.8869513273239136\n",
      "Epoch 307/750, Training Loss: 1.0328933000564575, Validation Loss: 0.8864413499832153\n",
      "Epoch 308/750, Training Loss: 1.0387332439422607, Validation Loss: 0.88592529296875\n",
      "Epoch 309/750, Training Loss: 1.0305564403533936, Validation Loss: 0.8853966593742371\n",
      "Epoch 310/750, Training Loss: 1.0308247804641724, Validation Loss: 0.884854793548584\n",
      "Epoch 311/750, Training Loss: 1.0329813957214355, Validation Loss: 0.884309709072113\n",
      "Epoch 312/750, Training Loss: 1.0282033681869507, Validation Loss: 0.8837351202964783\n",
      "Epoch 313/750, Training Loss: 1.031217098236084, Validation Loss: 0.8831527233123779\n",
      "Epoch 314/750, Training Loss: 1.0263034105300903, Validation Loss: 0.8825693726539612\n",
      "Epoch 315/750, Training Loss: 1.0290452241897583, Validation Loss: 0.8819712400436401\n",
      "Epoch 316/750, Training Loss: 1.0268138647079468, Validation Loss: 0.88141268491745\n",
      "Epoch 317/750, Training Loss: 1.018208384513855, Validation Loss: 0.8808640241622925\n",
      "Epoch 318/750, Training Loss: 1.0203124284744263, Validation Loss: 0.8803297877311707\n",
      "Epoch 319/750, Training Loss: 1.0237900018692017, Validation Loss: 0.8797900080680847\n",
      "Epoch 320/750, Training Loss: 1.0169535875320435, Validation Loss: 0.8792516589164734\n",
      "Epoch 321/750, Training Loss: 1.0194365978240967, Validation Loss: 0.8787436485290527\n",
      "Epoch 322/750, Training Loss: 1.0239245891571045, Validation Loss: 0.8782733678817749\n",
      "Epoch 323/750, Training Loss: 1.015057921409607, Validation Loss: 0.8778141736984253\n",
      "Epoch 324/750, Training Loss: 1.018996000289917, Validation Loss: 0.8773574233055115\n",
      "Epoch 325/750, Training Loss: 1.0160001516342163, Validation Loss: 0.8769297003746033\n",
      "Epoch 326/750, Training Loss: 1.020744800567627, Validation Loss: 0.8764966130256653\n",
      "Epoch 327/750, Training Loss: 1.0199456214904785, Validation Loss: 0.8760527968406677\n",
      "Epoch 328/750, Training Loss: 1.0168818235397339, Validation Loss: 0.8755723237991333\n",
      "Epoch 329/750, Training Loss: 1.0187684297561646, Validation Loss: 0.8751257061958313\n",
      "Epoch 330/750, Training Loss: 1.016005516052246, Validation Loss: 0.8746908903121948\n",
      "Epoch 331/750, Training Loss: 1.0167242288589478, Validation Loss: 0.8742296099662781\n",
      "Epoch 332/750, Training Loss: 1.0074317455291748, Validation Loss: 0.8737635016441345\n",
      "Epoch 333/750, Training Loss: 1.0155236721038818, Validation Loss: 0.8732832670211792\n",
      "Epoch 334/750, Training Loss: 1.0153237581253052, Validation Loss: 0.8728076219558716\n",
      "Epoch 335/750, Training Loss: 1.012061595916748, Validation Loss: 0.8723562955856323\n",
      "Epoch 336/750, Training Loss: 1.0110961198806763, Validation Loss: 0.8718822002410889\n",
      "Epoch 337/750, Training Loss: 1.0043046474456787, Validation Loss: 0.8713746666908264\n",
      "Epoch 338/750, Training Loss: 1.0094002485275269, Validation Loss: 0.8708996176719666\n",
      "Epoch 339/750, Training Loss: 1.0025417804718018, Validation Loss: 0.8704481720924377\n",
      "Epoch 340/750, Training Loss: 1.0082324743270874, Validation Loss: 0.8700000643730164\n",
      "Epoch 341/750, Training Loss: 1.001407265663147, Validation Loss: 0.869564950466156\n",
      "Epoch 342/750, Training Loss: 1.0070674419403076, Validation Loss: 0.8691146373748779\n",
      "Epoch 343/750, Training Loss: 1.0042812824249268, Validation Loss: 0.8686652183532715\n",
      "Epoch 344/750, Training Loss: 1.001095175743103, Validation Loss: 0.8682422041893005\n",
      "Epoch 345/750, Training Loss: 1.001065731048584, Validation Loss: 0.8677831292152405\n",
      "Epoch 346/750, Training Loss: 1.0043972730636597, Validation Loss: 0.8673243522644043\n",
      "Epoch 347/750, Training Loss: 0.996847927570343, Validation Loss: 0.8668731451034546\n",
      "Epoch 348/750, Training Loss: 1.0019429922103882, Validation Loss: 0.8664450645446777\n",
      "Epoch 349/750, Training Loss: 0.9974687695503235, Validation Loss: 0.866045355796814\n",
      "Epoch 350/750, Training Loss: 1.0016915798187256, Validation Loss: 0.8656401038169861\n",
      "Epoch 351/750, Training Loss: 1.0004061460494995, Validation Loss: 0.8652302026748657\n",
      "Epoch 352/750, Training Loss: 1.0002495050430298, Validation Loss: 0.8648528456687927\n",
      "Epoch 353/750, Training Loss: 0.9946085214614868, Validation Loss: 0.8644492626190186\n",
      "Epoch 354/750, Training Loss: 0.9942946434020996, Validation Loss: 0.8640628457069397\n",
      "Epoch 355/750, Training Loss: 0.9956361055374146, Validation Loss: 0.863669216632843\n",
      "Epoch 356/750, Training Loss: 0.9980528354644775, Validation Loss: 0.8632758259773254\n",
      "Epoch 357/750, Training Loss: 0.9995951652526855, Validation Loss: 0.8629231452941895\n",
      "Epoch 358/750, Training Loss: 0.9998137354850769, Validation Loss: 0.8625383973121643\n",
      "Epoch 359/750, Training Loss: 0.996456503868103, Validation Loss: 0.8621646165847778\n",
      "Epoch 360/750, Training Loss: 0.997994601726532, Validation Loss: 0.8617806434631348\n",
      "Epoch 361/750, Training Loss: 0.9919221997261047, Validation Loss: 0.86139315366745\n",
      "Epoch 362/750, Training Loss: 0.9898996353149414, Validation Loss: 0.8610026836395264\n",
      "Epoch 363/750, Training Loss: 0.9911087155342102, Validation Loss: 0.8605826497077942\n",
      "Epoch 364/750, Training Loss: 0.9864298105239868, Validation Loss: 0.8602027297019958\n",
      "Epoch 365/750, Training Loss: 0.9887197613716125, Validation Loss: 0.8598247170448303\n",
      "Epoch 366/750, Training Loss: 0.9913344979286194, Validation Loss: 0.859413206577301\n",
      "Epoch 367/750, Training Loss: 0.9899400472640991, Validation Loss: 0.8589956760406494\n",
      "Epoch 368/750, Training Loss: 0.9884535670280457, Validation Loss: 0.8585755825042725\n",
      "Epoch 369/750, Training Loss: 0.9916375279426575, Validation Loss: 0.8581885695457458\n",
      "Epoch 370/750, Training Loss: 0.9859016537666321, Validation Loss: 0.8578175902366638\n",
      "Epoch 371/750, Training Loss: 0.9863375425338745, Validation Loss: 0.8574400544166565\n",
      "Epoch 372/750, Training Loss: 0.9881507158279419, Validation Loss: 0.857090175151825\n",
      "Epoch 373/750, Training Loss: 0.9863609075546265, Validation Loss: 0.8567423820495605\n",
      "Epoch 374/750, Training Loss: 0.9799394011497498, Validation Loss: 0.8563960194587708\n",
      "Epoch 375/750, Training Loss: 0.982890248298645, Validation Loss: 0.8560549020767212\n",
      "Epoch 376/750, Training Loss: 0.9764538407325745, Validation Loss: 0.8557003140449524\n",
      "Epoch 377/750, Training Loss: 0.9818903803825378, Validation Loss: 0.8553342819213867\n",
      "Epoch 378/750, Training Loss: 0.9797744154930115, Validation Loss: 0.8549816012382507\n",
      "Epoch 379/750, Training Loss: 0.9787902235984802, Validation Loss: 0.8546264171600342\n",
      "Epoch 380/750, Training Loss: 0.9855598211288452, Validation Loss: 0.8542634844779968\n",
      "Epoch 381/750, Training Loss: 0.9883108735084534, Validation Loss: 0.853912353515625\n",
      "Epoch 382/750, Training Loss: 0.9777067303657532, Validation Loss: 0.8535619974136353\n",
      "Epoch 383/750, Training Loss: 0.9744377136230469, Validation Loss: 0.85323566198349\n",
      "Epoch 384/750, Training Loss: 0.9790467023849487, Validation Loss: 0.852912425994873\n",
      "Epoch 385/750, Training Loss: 0.9757905006408691, Validation Loss: 0.8525772094726562\n",
      "Epoch 386/750, Training Loss: 0.9765552878379822, Validation Loss: 0.8522300720214844\n",
      "Epoch 387/750, Training Loss: 0.9766235947608948, Validation Loss: 0.851919412612915\n",
      "Epoch 388/750, Training Loss: 0.9765963554382324, Validation Loss: 0.851625919342041\n",
      "Epoch 389/750, Training Loss: 0.9796688556671143, Validation Loss: 0.8513246774673462\n",
      "Epoch 390/750, Training Loss: 0.9716028571128845, Validation Loss: 0.8510202765464783\n",
      "Epoch 391/750, Training Loss: 0.9726635813713074, Validation Loss: 0.8507264256477356\n",
      "Epoch 392/750, Training Loss: 0.9680177569389343, Validation Loss: 0.8504405617713928\n",
      "Epoch 393/750, Training Loss: 0.9651517868041992, Validation Loss: 0.8501383662223816\n",
      "Epoch 394/750, Training Loss: 0.968842089176178, Validation Loss: 0.8498566746711731\n",
      "Epoch 395/750, Training Loss: 0.9707725048065186, Validation Loss: 0.8495720624923706\n",
      "Epoch 396/750, Training Loss: 0.9707082509994507, Validation Loss: 0.8492435216903687\n",
      "Epoch 397/750, Training Loss: 0.9713242053985596, Validation Loss: 0.848896324634552\n",
      "Epoch 398/750, Training Loss: 0.9646177291870117, Validation Loss: 0.8485295176506042\n",
      "Epoch 399/750, Training Loss: 0.9624837636947632, Validation Loss: 0.8481655120849609\n",
      "Epoch 400/750, Training Loss: 0.9645612239837646, Validation Loss: 0.8477885127067566\n",
      "Epoch 401/750, Training Loss: 0.9688147306442261, Validation Loss: 0.8474240303039551\n",
      "Epoch 402/750, Training Loss: 0.9677754640579224, Validation Loss: 0.8470553755760193\n",
      "Epoch 403/750, Training Loss: 0.9677820801734924, Validation Loss: 0.8466898202896118\n",
      "Epoch 404/750, Training Loss: 0.9608222842216492, Validation Loss: 0.8463183045387268\n",
      "Epoch 405/750, Training Loss: 0.9682210683822632, Validation Loss: 0.8459502458572388\n",
      "Epoch 406/750, Training Loss: 0.9690011143684387, Validation Loss: 0.8455813527107239\n",
      "Epoch 407/750, Training Loss: 0.9670215249061584, Validation Loss: 0.8452601432800293\n",
      "Epoch 408/750, Training Loss: 0.9636062383651733, Validation Loss: 0.8449741005897522\n",
      "Epoch 409/750, Training Loss: 0.9579879641532898, Validation Loss: 0.8447317481040955\n",
      "Epoch 410/750, Training Loss: 0.957370400428772, Validation Loss: 0.8445091247558594\n",
      "Epoch 411/750, Training Loss: 0.9618008136749268, Validation Loss: 0.8442678451538086\n",
      "Epoch 412/750, Training Loss: 0.961597204208374, Validation Loss: 0.844001054763794\n",
      "Epoch 413/750, Training Loss: 0.9619323015213013, Validation Loss: 0.8437414765357971\n",
      "Epoch 414/750, Training Loss: 0.9557256698608398, Validation Loss: 0.8434606194496155\n",
      "Epoch 415/750, Training Loss: 0.9661921858787537, Validation Loss: 0.8431698679924011\n",
      "Epoch 416/750, Training Loss: 0.962448000907898, Validation Loss: 0.8428995609283447\n",
      "Epoch 417/750, Training Loss: 0.9549672603607178, Validation Loss: 0.8426300287246704\n",
      "Epoch 418/750, Training Loss: 0.9555529952049255, Validation Loss: 0.842343270778656\n",
      "Epoch 419/750, Training Loss: 0.9576085209846497, Validation Loss: 0.8420448303222656\n",
      "Epoch 420/750, Training Loss: 0.955180823802948, Validation Loss: 0.8417337536811829\n",
      "Epoch 421/750, Training Loss: 0.9551747441291809, Validation Loss: 0.8414461612701416\n",
      "Epoch 422/750, Training Loss: 0.9565252065658569, Validation Loss: 0.8411793112754822\n",
      "Epoch 423/750, Training Loss: 0.952788233757019, Validation Loss: 0.8409023284912109\n",
      "Epoch 424/750, Training Loss: 0.9530584812164307, Validation Loss: 0.8406208753585815\n",
      "Epoch 425/750, Training Loss: 0.9459444880485535, Validation Loss: 0.840328574180603\n",
      "Epoch 426/750, Training Loss: 0.9519813060760498, Validation Loss: 0.8400322198867798\n",
      "Epoch 427/750, Training Loss: 0.949590265750885, Validation Loss: 0.8397499918937683\n",
      "Epoch 428/750, Training Loss: 0.953438937664032, Validation Loss: 0.8394957184791565\n",
      "Epoch 429/750, Training Loss: 0.9531593322753906, Validation Loss: 0.8392387628555298\n",
      "Epoch 430/750, Training Loss: 0.9480305314064026, Validation Loss: 0.8389583230018616\n",
      "Epoch 431/750, Training Loss: 0.947130560874939, Validation Loss: 0.8386853337287903\n",
      "Epoch 432/750, Training Loss: 0.9460045695304871, Validation Loss: 0.8383967280387878\n",
      "Epoch 433/750, Training Loss: 0.9500126838684082, Validation Loss: 0.8381524085998535\n",
      "Epoch 434/750, Training Loss: 0.9432096481323242, Validation Loss: 0.8379221558570862\n",
      "Epoch 435/750, Training Loss: 0.9499700665473938, Validation Loss: 0.8376707434654236\n",
      "Epoch 436/750, Training Loss: 0.9440074563026428, Validation Loss: 0.8374232053756714\n",
      "Epoch 437/750, Training Loss: 0.9491880536079407, Validation Loss: 0.837214469909668\n",
      "Epoch 438/750, Training Loss: 0.9415464401245117, Validation Loss: 0.8369980454444885\n",
      "Epoch 439/750, Training Loss: 0.9469518661499023, Validation Loss: 0.8367415070533752\n",
      "Epoch 440/750, Training Loss: 0.9456288814544678, Validation Loss: 0.8364846706390381\n",
      "Epoch 441/750, Training Loss: 0.9440810680389404, Validation Loss: 0.8362282514572144\n",
      "Epoch 442/750, Training Loss: 0.9404336810112, Validation Loss: 0.8359661102294922\n",
      "Epoch 443/750, Training Loss: 0.9447662234306335, Validation Loss: 0.8357158899307251\n",
      "Epoch 444/750, Training Loss: 0.9408083558082581, Validation Loss: 0.8354673981666565\n",
      "Epoch 445/750, Training Loss: 0.9329793453216553, Validation Loss: 0.8352252840995789\n",
      "Epoch 446/750, Training Loss: 0.9481962323188782, Validation Loss: 0.8349930047988892\n",
      "Epoch 447/750, Training Loss: 0.9367324709892273, Validation Loss: 0.8347485661506653\n",
      "Epoch 448/750, Training Loss: 0.9397718906402588, Validation Loss: 0.8344980478286743\n",
      "Epoch 449/750, Training Loss: 0.940899670124054, Validation Loss: 0.8342727422714233\n",
      "Epoch 450/750, Training Loss: 0.9373078346252441, Validation Loss: 0.8340405821800232\n",
      "Epoch 451/750, Training Loss: 0.9386069178581238, Validation Loss: 0.8338143825531006\n",
      "Epoch 452/750, Training Loss: 0.9371191263198853, Validation Loss: 0.833575427532196\n",
      "Epoch 453/750, Training Loss: 0.9375529885292053, Validation Loss: 0.8333139419555664\n",
      "Epoch 454/750, Training Loss: 0.9444179534912109, Validation Loss: 0.8330631256103516\n",
      "Epoch 455/750, Training Loss: 0.9310482740402222, Validation Loss: 0.8328385949134827\n",
      "Epoch 456/750, Training Loss: 0.9417627453804016, Validation Loss: 0.8326205015182495\n",
      "Epoch 457/750, Training Loss: 0.9380227327346802, Validation Loss: 0.8323764801025391\n",
      "Epoch 458/750, Training Loss: 0.9282084107398987, Validation Loss: 0.8321357369422913\n",
      "Epoch 459/750, Training Loss: 0.9314296245574951, Validation Loss: 0.831917405128479\n",
      "Epoch 460/750, Training Loss: 0.9258867502212524, Validation Loss: 0.8317010402679443\n",
      "Epoch 461/750, Training Loss: 0.9285737872123718, Validation Loss: 0.8314829468727112\n",
      "Epoch 462/750, Training Loss: 0.9297876358032227, Validation Loss: 0.8313031196594238\n",
      "Epoch 463/750, Training Loss: 0.9297429323196411, Validation Loss: 0.8311429619789124\n",
      "Epoch 464/750, Training Loss: 0.934949517250061, Validation Loss: 0.8309776782989502\n",
      "Epoch 465/750, Training Loss: 0.9337959289550781, Validation Loss: 0.8307868838310242\n",
      "Epoch 466/750, Training Loss: 0.9302586317062378, Validation Loss: 0.8305729627609253\n",
      "Epoch 467/750, Training Loss: 0.9313755631446838, Validation Loss: 0.830372154712677\n",
      "Epoch 468/750, Training Loss: 0.9242206811904907, Validation Loss: 0.8301588296890259\n",
      "Epoch 469/750, Training Loss: 0.924350380897522, Validation Loss: 0.8299617767333984\n",
      "Epoch 470/750, Training Loss: 0.919472873210907, Validation Loss: 0.8297317624092102\n",
      "Epoch 471/750, Training Loss: 0.9262939095497131, Validation Loss: 0.8294429183006287\n",
      "Epoch 472/750, Training Loss: 0.9230701923370361, Validation Loss: 0.8291419744491577\n",
      "Epoch 473/750, Training Loss: 0.9305397272109985, Validation Loss: 0.8288601636886597\n",
      "Epoch 474/750, Training Loss: 0.92415851354599, Validation Loss: 0.8285491466522217\n",
      "Epoch 475/750, Training Loss: 0.924793004989624, Validation Loss: 0.8282451629638672\n",
      "Epoch 476/750, Training Loss: 0.9248310327529907, Validation Loss: 0.8279802203178406\n",
      "Epoch 477/750, Training Loss: 0.9197319746017456, Validation Loss: 0.8277047872543335\n",
      "Epoch 478/750, Training Loss: 0.923208475112915, Validation Loss: 0.8274631500244141\n",
      "Epoch 479/750, Training Loss: 0.9240522384643555, Validation Loss: 0.8272503018379211\n",
      "Epoch 480/750, Training Loss: 0.9224952459335327, Validation Loss: 0.8270241618156433\n",
      "Epoch 481/750, Training Loss: 0.9225550889968872, Validation Loss: 0.8268116116523743\n",
      "Epoch 482/750, Training Loss: 0.9250251054763794, Validation Loss: 0.8266543745994568\n",
      "Epoch 483/750, Training Loss: 0.9253433346748352, Validation Loss: 0.8265202641487122\n",
      "Epoch 484/750, Training Loss: 0.9253224730491638, Validation Loss: 0.8263773918151855\n",
      "Epoch 485/750, Training Loss: 0.9200137853622437, Validation Loss: 0.8262617588043213\n",
      "Epoch 486/750, Training Loss: 0.9191149473190308, Validation Loss: 0.826164186000824\n",
      "Epoch 487/750, Training Loss: 0.9162235856056213, Validation Loss: 0.8260766267776489\n",
      "Epoch 488/750, Training Loss: 0.9210475087165833, Validation Loss: 0.8259934186935425\n",
      "Epoch 489/750, Training Loss: 0.9172958731651306, Validation Loss: 0.8258779048919678\n",
      "Epoch 490/750, Training Loss: 0.9217866063117981, Validation Loss: 0.8257013559341431\n",
      "Epoch 491/750, Training Loss: 0.9167128801345825, Validation Loss: 0.825551450252533\n",
      "Epoch 492/750, Training Loss: 0.9168227910995483, Validation Loss: 0.8253603577613831\n",
      "Epoch 493/750, Training Loss: 0.9146687388420105, Validation Loss: 0.8251451849937439\n",
      "Epoch 494/750, Training Loss: 0.9107770323753357, Validation Loss: 0.8248910307884216\n",
      "Epoch 495/750, Training Loss: 0.9152581095695496, Validation Loss: 0.8246238827705383\n",
      "Epoch 496/750, Training Loss: 0.913379430770874, Validation Loss: 0.8243461847305298\n",
      "Epoch 497/750, Training Loss: 0.9150511622428894, Validation Loss: 0.8240716457366943\n",
      "Epoch 498/750, Training Loss: 0.9224461317062378, Validation Loss: 0.8238478899002075\n",
      "Epoch 499/750, Training Loss: 0.9132130146026611, Validation Loss: 0.8236358165740967\n",
      "Epoch 500/750, Training Loss: 0.9184261560440063, Validation Loss: 0.823457658290863\n",
      "Epoch 501/750, Training Loss: 0.9124192595481873, Validation Loss: 0.823282778263092\n",
      "Epoch 502/750, Training Loss: 0.9136456847190857, Validation Loss: 0.8231146931648254\n",
      "Epoch 503/750, Training Loss: 0.9149724841117859, Validation Loss: 0.8229735493659973\n",
      "Epoch 504/750, Training Loss: 0.9107696413993835, Validation Loss: 0.8228131532669067\n",
      "Epoch 505/750, Training Loss: 0.9138145446777344, Validation Loss: 0.8226627111434937\n",
      "Epoch 506/750, Training Loss: 0.9170458912849426, Validation Loss: 0.8225337266921997\n",
      "Epoch 507/750, Training Loss: 0.905115008354187, Validation Loss: 0.8224216103553772\n",
      "Epoch 508/750, Training Loss: 0.9078176021575928, Validation Loss: 0.8223282694816589\n",
      "Epoch 509/750, Training Loss: 0.912844181060791, Validation Loss: 0.8222353458404541\n",
      "Epoch 510/750, Training Loss: 0.9124844670295715, Validation Loss: 0.8221414089202881\n",
      "Epoch 511/750, Training Loss: 0.9072933197021484, Validation Loss: 0.8219918608665466\n",
      "Epoch 512/750, Training Loss: 0.9100781083106995, Validation Loss: 0.8218342661857605\n",
      "Epoch 513/750, Training Loss: 0.9080117344856262, Validation Loss: 0.8216715455055237\n",
      "Epoch 514/750, Training Loss: 0.9055911302566528, Validation Loss: 0.8214664459228516\n",
      "Epoch 515/750, Training Loss: 0.9057884812355042, Validation Loss: 0.8212772607803345\n",
      "Epoch 516/750, Training Loss: 0.9078545570373535, Validation Loss: 0.8210529685020447\n",
      "Epoch 517/750, Training Loss: 0.902239978313446, Validation Loss: 0.8208116888999939\n",
      "Epoch 518/750, Training Loss: 0.9018511772155762, Validation Loss: 0.8205769062042236\n",
      "Epoch 519/750, Training Loss: 0.9088037610054016, Validation Loss: 0.8203744292259216\n",
      "Epoch 520/750, Training Loss: 0.9061470627784729, Validation Loss: 0.8201741576194763\n",
      "Epoch 521/750, Training Loss: 0.9029626846313477, Validation Loss: 0.8199794292449951\n",
      "Epoch 522/750, Training Loss: 0.9019221663475037, Validation Loss: 0.819811999797821\n",
      "Epoch 523/750, Training Loss: 0.9008100032806396, Validation Loss: 0.8196528553962708\n",
      "Epoch 524/750, Training Loss: 0.9007609486579895, Validation Loss: 0.8195332288742065\n",
      "Epoch 525/750, Training Loss: 0.9043996334075928, Validation Loss: 0.8194507956504822\n",
      "Epoch 526/750, Training Loss: 0.907913088798523, Validation Loss: 0.8193363547325134\n",
      "Epoch 527/750, Training Loss: 0.9022544622421265, Validation Loss: 0.8192562460899353\n",
      "Epoch 528/750, Training Loss: 0.8982627391815186, Validation Loss: 0.8191988468170166\n",
      "Epoch 529/750, Training Loss: 0.8986670970916748, Validation Loss: 0.819090723991394\n",
      "Epoch 530/750, Training Loss: 0.9011833071708679, Validation Loss: 0.8189354538917542\n",
      "Epoch 531/750, Training Loss: 0.8999074697494507, Validation Loss: 0.8187251687049866\n",
      "Epoch 532/750, Training Loss: 0.8997094035148621, Validation Loss: 0.8184757828712463\n",
      "Epoch 533/750, Training Loss: 0.9044756889343262, Validation Loss: 0.8182209134101868\n",
      "Epoch 534/750, Training Loss: 0.8986027240753174, Validation Loss: 0.817983090877533\n",
      "Epoch 535/750, Training Loss: 0.9015662670135498, Validation Loss: 0.8178023099899292\n",
      "Epoch 536/750, Training Loss: 0.9042783379554749, Validation Loss: 0.8176701068878174\n",
      "Epoch 537/750, Training Loss: 0.8925226926803589, Validation Loss: 0.8175610899925232\n",
      "Epoch 538/750, Training Loss: 0.8949857950210571, Validation Loss: 0.8174818754196167\n",
      "Epoch 539/750, Training Loss: 0.900397539138794, Validation Loss: 0.8174377679824829\n",
      "Epoch 540/750, Training Loss: 0.8984139561653137, Validation Loss: 0.8174185156822205\n",
      "Epoch 541/750, Training Loss: 0.9014871120452881, Validation Loss: 0.8173707127571106\n",
      "Epoch 542/750, Training Loss: 0.896874189376831, Validation Loss: 0.817275881767273\n",
      "Epoch 543/750, Training Loss: 0.8994626998901367, Validation Loss: 0.8171809315681458\n",
      "Epoch 544/750, Training Loss: 0.8954547643661499, Validation Loss: 0.8170405626296997\n",
      "Epoch 545/750, Training Loss: 0.8957672715187073, Validation Loss: 0.81685870885849\n",
      "Epoch 546/750, Training Loss: 0.8932675123214722, Validation Loss: 0.8166201710700989\n",
      "Epoch 547/750, Training Loss: 0.8903519511222839, Validation Loss: 0.8163824677467346\n",
      "Epoch 548/750, Training Loss: 0.8930450677871704, Validation Loss: 0.8161581158638\n",
      "Epoch 549/750, Training Loss: 0.8906961679458618, Validation Loss: 0.8159188628196716\n",
      "Epoch 550/750, Training Loss: 0.8904465436935425, Validation Loss: 0.8157240152359009\n",
      "Epoch 551/750, Training Loss: 0.8955993056297302, Validation Loss: 0.8155633211135864\n",
      "Epoch 552/750, Training Loss: 0.8925495147705078, Validation Loss: 0.8154553174972534\n",
      "Epoch 553/750, Training Loss: 0.8923563361167908, Validation Loss: 0.8153749108314514\n",
      "Epoch 554/750, Training Loss: 0.8914871215820312, Validation Loss: 0.8153011202812195\n",
      "Epoch 555/750, Training Loss: 0.8906808495521545, Validation Loss: 0.8152802586555481\n",
      "Epoch 556/750, Training Loss: 0.8852182626724243, Validation Loss: 0.8152269721031189\n",
      "Epoch 557/750, Training Loss: 0.8931225538253784, Validation Loss: 0.8151475787162781\n",
      "Epoch 558/750, Training Loss: 0.8870672583580017, Validation Loss: 0.8150473833084106\n",
      "Epoch 559/750, Training Loss: 0.889887809753418, Validation Loss: 0.8149204850196838\n",
      "Epoch 560/750, Training Loss: 0.8923722505569458, Validation Loss: 0.8147751092910767\n",
      "Epoch 561/750, Training Loss: 0.8861985206604004, Validation Loss: 0.8146592378616333\n",
      "Epoch 562/750, Training Loss: 0.8930277824401855, Validation Loss: 0.8145545125007629\n",
      "Epoch 563/750, Training Loss: 0.8901638388633728, Validation Loss: 0.8144280314445496\n",
      "Epoch 564/750, Training Loss: 0.8881955146789551, Validation Loss: 0.8142647743225098\n",
      "Epoch 565/750, Training Loss: 0.8839277029037476, Validation Loss: 0.8141862750053406\n",
      "Epoch 566/750, Training Loss: 0.8851413130760193, Validation Loss: 0.8141282796859741\n",
      "Epoch 567/750, Training Loss: 0.8806931972503662, Validation Loss: 0.814064085483551\n",
      "Epoch 568/750, Training Loss: 0.8798842430114746, Validation Loss: 0.8140019178390503\n",
      "Epoch 569/750, Training Loss: 0.8820126056671143, Validation Loss: 0.8139204382896423\n",
      "Epoch 570/750, Training Loss: 0.8831403851509094, Validation Loss: 0.8138608932495117\n",
      "Epoch 571/750, Training Loss: 0.888797402381897, Validation Loss: 0.8137659430503845\n",
      "Epoch 572/750, Training Loss: 0.8862151503562927, Validation Loss: 0.8136405348777771\n",
      "Epoch 573/750, Training Loss: 0.8811249732971191, Validation Loss: 0.8134816884994507\n",
      "Epoch 574/750, Training Loss: 0.8828955292701721, Validation Loss: 0.8132985830307007\n",
      "Epoch 575/750, Training Loss: 0.8853467106819153, Validation Loss: 0.8130974769592285\n",
      "Epoch 576/750, Training Loss: 0.8808309435844421, Validation Loss: 0.8128927946090698\n",
      "Epoch 577/750, Training Loss: 0.880718469619751, Validation Loss: 0.8127431869506836\n",
      "Epoch 578/750, Training Loss: 0.8830617666244507, Validation Loss: 0.8126273155212402\n",
      "Epoch 579/750, Training Loss: 0.8885645270347595, Validation Loss: 0.8125308156013489\n",
      "Epoch 580/750, Training Loss: 0.8797581195831299, Validation Loss: 0.8124508261680603\n",
      "Epoch 581/750, Training Loss: 0.8762856721878052, Validation Loss: 0.8123639225959778\n",
      "Epoch 582/750, Training Loss: 0.8806729912757874, Validation Loss: 0.8122886419296265\n",
      "Epoch 583/750, Training Loss: 0.8786866068840027, Validation Loss: 0.8122363090515137\n",
      "Epoch 584/750, Training Loss: 0.8716117143630981, Validation Loss: 0.8121886253356934\n",
      "Epoch 585/750, Training Loss: 0.8820130825042725, Validation Loss: 0.8121272325515747\n",
      "Epoch 586/750, Training Loss: 0.8767728209495544, Validation Loss: 0.8120348453521729\n",
      "Epoch 587/750, Training Loss: 0.879374086856842, Validation Loss: 0.8119527697563171\n",
      "Epoch 588/750, Training Loss: 0.8756195306777954, Validation Loss: 0.8118657469749451\n",
      "Epoch 589/750, Training Loss: 0.8733269572257996, Validation Loss: 0.8117762804031372\n",
      "Epoch 590/750, Training Loss: 0.8804284334182739, Validation Loss: 0.8116709589958191\n",
      "Epoch 591/750, Training Loss: 0.8758476972579956, Validation Loss: 0.8115729093551636\n",
      "Epoch 592/750, Training Loss: 0.8726509213447571, Validation Loss: 0.8114892840385437\n",
      "Epoch 593/750, Training Loss: 0.8805958032608032, Validation Loss: 0.8114163875579834\n",
      "Epoch 594/750, Training Loss: 0.8763667345046997, Validation Loss: 0.8113358616828918\n",
      "Epoch 595/750, Training Loss: 0.8720462322235107, Validation Loss: 0.8112083077430725\n",
      "Epoch 596/750, Training Loss: 0.8706023097038269, Validation Loss: 0.8110474348068237\n",
      "Epoch 597/750, Training Loss: 0.8692630529403687, Validation Loss: 0.8108780384063721\n",
      "Epoch 598/750, Training Loss: 0.870255708694458, Validation Loss: 0.8107170462608337\n",
      "Epoch 599/750, Training Loss: 0.8722484707832336, Validation Loss: 0.8105835318565369\n",
      "Epoch 600/750, Training Loss: 0.8742164373397827, Validation Loss: 0.8104555010795593\n",
      "Epoch 601/750, Training Loss: 0.8687295317649841, Validation Loss: 0.8103572726249695\n",
      "Epoch 602/750, Training Loss: 0.8690065741539001, Validation Loss: 0.8102942109107971\n",
      "Epoch 603/750, Training Loss: 0.8730345964431763, Validation Loss: 0.8102105855941772\n",
      "Epoch 604/750, Training Loss: 0.8711053133010864, Validation Loss: 0.810144305229187\n",
      "Epoch 605/750, Training Loss: 0.8690727949142456, Validation Loss: 0.8100453019142151\n",
      "Epoch 606/750, Training Loss: 0.8725526332855225, Validation Loss: 0.8099520802497864\n",
      "Epoch 607/750, Training Loss: 0.8670423030853271, Validation Loss: 0.8098918795585632\n",
      "Epoch 608/750, Training Loss: 0.8721218705177307, Validation Loss: 0.8098145723342896\n",
      "Epoch 609/750, Training Loss: 0.8688678741455078, Validation Loss: 0.8096987009048462\n",
      "Epoch 610/750, Training Loss: 0.8689665794372559, Validation Loss: 0.8095613121986389\n",
      "Epoch 611/750, Training Loss: 0.8681370615959167, Validation Loss: 0.8094451427459717\n",
      "Epoch 612/750, Training Loss: 0.865595281124115, Validation Loss: 0.8093190789222717\n",
      "Epoch 613/750, Training Loss: 0.8680346608161926, Validation Loss: 0.8091994524002075\n",
      "Epoch 614/750, Training Loss: 0.8664681911468506, Validation Loss: 0.8090930581092834\n",
      "Epoch 615/750, Training Loss: 0.8656573295593262, Validation Loss: 0.8090155720710754\n",
      "Epoch 616/750, Training Loss: 0.8722687363624573, Validation Loss: 0.808932900428772\n",
      "Epoch 617/750, Training Loss: 0.8662015199661255, Validation Loss: 0.8088247776031494\n",
      "Epoch 618/750, Training Loss: 0.8694522380828857, Validation Loss: 0.8087268471717834\n",
      "Epoch 619/750, Training Loss: 0.8622353672981262, Validation Loss: 0.8086662292480469\n",
      "Epoch 620/750, Training Loss: 0.8673928380012512, Validation Loss: 0.8086268305778503\n",
      "Epoch 621/750, Training Loss: 0.8682326674461365, Validation Loss: 0.8085951209068298\n",
      "Epoch 622/750, Training Loss: 0.8593087196350098, Validation Loss: 0.8085848093032837\n",
      "Epoch 623/750, Training Loss: 0.8609203100204468, Validation Loss: 0.8085835576057434\n",
      "Epoch 624/750, Training Loss: 0.8639664649963379, Validation Loss: 0.8085912466049194\n",
      "Epoch 625/750, Training Loss: 0.8678231239318848, Validation Loss: 0.808613121509552\n",
      "Epoch 626/750, Training Loss: 0.86739182472229, Validation Loss: 0.8085936307907104\n",
      "Epoch 627/750, Training Loss: 0.8643887042999268, Validation Loss: 0.8085530400276184\n",
      "Epoch 628/750, Training Loss: 0.8637002110481262, Validation Loss: 0.8085666298866272\n",
      "Epoch 629/750, Training Loss: 0.8650763630867004, Validation Loss: 0.8085658550262451\n",
      "Epoch 630/750, Training Loss: 0.8666101694107056, Validation Loss: 0.8085682988166809\n",
      "Epoch 631/750, Training Loss: 0.860308051109314, Validation Loss: 0.8085192441940308\n",
      "Epoch 632/750, Training Loss: 0.8620377779006958, Validation Loss: 0.8084269165992737\n",
      "Epoch 633/750, Training Loss: 0.8559338450431824, Validation Loss: 0.808297872543335\n",
      "Epoch 634/750, Training Loss: 0.8641052842140198, Validation Loss: 0.8082119226455688\n",
      "Epoch 635/750, Training Loss: 0.860713541507721, Validation Loss: 0.8081380724906921\n",
      "Epoch 636/750, Training Loss: 0.8614619970321655, Validation Loss: 0.8080495595932007\n",
      "Epoch 637/750, Training Loss: 0.8653547763824463, Validation Loss: 0.8079331517219543\n",
      "Epoch 638/750, Training Loss: 0.8646695017814636, Validation Loss: 0.8078332543373108\n",
      "Epoch 639/750, Training Loss: 0.8572368025779724, Validation Loss: 0.8077218532562256\n",
      "Epoch 640/750, Training Loss: 0.8560264110565186, Validation Loss: 0.8075945377349854\n",
      "Epoch 641/750, Training Loss: 0.8566864132881165, Validation Loss: 0.8074541091918945\n",
      "Epoch 642/750, Training Loss: 0.8543999791145325, Validation Loss: 0.8073282837867737\n",
      "Epoch 643/750, Training Loss: 0.8587707281112671, Validation Loss: 0.8072090148925781\n",
      "Epoch 644/750, Training Loss: 0.8570563197135925, Validation Loss: 0.8070988059043884\n",
      "Epoch 645/750, Training Loss: 0.8575994968414307, Validation Loss: 0.8069880604743958\n",
      "Epoch 646/750, Training Loss: 0.8567696213722229, Validation Loss: 0.8069096803665161\n",
      "Epoch 647/750, Training Loss: 0.8530437350273132, Validation Loss: 0.8068404793739319\n",
      "Epoch 648/750, Training Loss: 0.8553390502929688, Validation Loss: 0.8067717552185059\n",
      "Epoch 649/750, Training Loss: 0.8526486158370972, Validation Loss: 0.8067277669906616\n",
      "Epoch 650/750, Training Loss: 0.8575451374053955, Validation Loss: 0.8067141771316528\n",
      "Epoch 651/750, Training Loss: 0.8539261817932129, Validation Loss: 0.8067176938056946\n",
      "Epoch 652/750, Training Loss: 0.8558815717697144, Validation Loss: 0.8067363500595093\n",
      "Epoch 653/750, Training Loss: 0.849344789981842, Validation Loss: 0.8067359924316406\n",
      "Epoch 654/750, Training Loss: 0.8517663478851318, Validation Loss: 0.8067149519920349\n",
      "Epoch 655/750, Training Loss: 0.849507212638855, Validation Loss: 0.8066548109054565\n",
      "Epoch 656/750, Training Loss: 0.8494435548782349, Validation Loss: 0.8065721988677979\n",
      "Epoch 657/750, Training Loss: 0.8497105836868286, Validation Loss: 0.8064754009246826\n",
      "Epoch 658/750, Training Loss: 0.8555097579956055, Validation Loss: 0.8063827157020569\n",
      "Epoch 659/750, Training Loss: 0.8532508611679077, Validation Loss: 0.8063053488731384\n",
      "Epoch 660/750, Training Loss: 0.8511015176773071, Validation Loss: 0.8062324523925781\n",
      "Epoch 661/750, Training Loss: 0.8526466488838196, Validation Loss: 0.8061792254447937\n",
      "Epoch 662/750, Training Loss: 0.8522201776504517, Validation Loss: 0.806135892868042\n",
      "Epoch 663/750, Training Loss: 0.8502656817436218, Validation Loss: 0.8061255812644958\n",
      "Epoch 664/750, Training Loss: 0.8487500548362732, Validation Loss: 0.8061190843582153\n",
      "Epoch 665/750, Training Loss: 0.8490763306617737, Validation Loss: 0.8061155676841736\n",
      "Epoch 666/750, Training Loss: 0.848536491394043, Validation Loss: 0.8060681819915771\n",
      "Epoch 667/750, Training Loss: 0.8514094948768616, Validation Loss: 0.8060262203216553\n",
      "Epoch 668/750, Training Loss: 0.8546172380447388, Validation Loss: 0.8059320449829102\n",
      "Epoch 669/750, Training Loss: 0.8453807830810547, Validation Loss: 0.8058736324310303\n",
      "Epoch 670/750, Training Loss: 0.8499220013618469, Validation Loss: 0.8058246970176697\n",
      "Epoch 671/750, Training Loss: 0.8508061766624451, Validation Loss: 0.805769681930542\n",
      "Epoch 672/750, Training Loss: 0.8475181460380554, Validation Loss: 0.8056829571723938\n",
      "Epoch 673/750, Training Loss: 0.8488771319389343, Validation Loss: 0.8056244850158691\n",
      "Epoch 674/750, Training Loss: 0.8456421494483948, Validation Loss: 0.8055539727210999\n",
      "Epoch 675/750, Training Loss: 0.8490399122238159, Validation Loss: 0.8054250478744507\n",
      "Epoch 676/750, Training Loss: 0.8490848541259766, Validation Loss: 0.8052855134010315\n",
      "Epoch 677/750, Training Loss: 0.8480825424194336, Validation Loss: 0.8051646947860718\n",
      "Epoch 678/750, Training Loss: 0.8428696393966675, Validation Loss: 0.8050371408462524\n",
      "Epoch 679/750, Training Loss: 0.8448441624641418, Validation Loss: 0.8049570322036743\n",
      "Epoch 680/750, Training Loss: 0.8486229777336121, Validation Loss: 0.8049134612083435\n",
      "Epoch 681/750, Training Loss: 0.845366358757019, Validation Loss: 0.8049173355102539\n",
      "Epoch 682/750, Training Loss: 0.8421861529350281, Validation Loss: 0.8049588799476624\n",
      "Epoch 683/750, Training Loss: 0.8444744944572449, Validation Loss: 0.8049917817115784\n",
      "Epoch 684/750, Training Loss: 0.8428829312324524, Validation Loss: 0.8050490617752075\n",
      "Epoch 685/750, Training Loss: 0.8422202467918396, Validation Loss: 0.8050684928894043\n",
      "Epoch 686/750, Training Loss: 0.8466323614120483, Validation Loss: 0.8050742149353027\n",
      "Epoch 687/750, Training Loss: 0.8402770757675171, Validation Loss: 0.8050467371940613\n",
      "Epoch 688/750, Training Loss: 0.8422417640686035, Validation Loss: 0.8049994111061096\n",
      "Epoch 689/750, Training Loss: 0.8422600626945496, Validation Loss: 0.8049440979957581\n",
      "Epoch 690/750, Training Loss: 0.8462832570075989, Validation Loss: 0.8048794865608215\n",
      "Epoch 691/750, Training Loss: 0.8382482528686523, Validation Loss: 0.8047732710838318\n",
      "Epoch 692/750, Training Loss: 0.8406018018722534, Validation Loss: 0.8046581149101257\n",
      "Epoch 693/750, Training Loss: 0.8489134311676025, Validation Loss: 0.8045344948768616\n",
      "Epoch 694/750, Training Loss: 0.8327949643135071, Validation Loss: 0.8043885827064514\n",
      "Epoch 695/750, Training Loss: 0.8444057703018188, Validation Loss: 0.8042961955070496\n",
      "Epoch 696/750, Training Loss: 0.8404474258422852, Validation Loss: 0.8042128682136536\n",
      "Epoch 697/750, Training Loss: 0.8414066433906555, Validation Loss: 0.8041516542434692\n",
      "Epoch 698/750, Training Loss: 0.843961238861084, Validation Loss: 0.8041002154350281\n",
      "Epoch 699/750, Training Loss: 0.8442689180374146, Validation Loss: 0.8040874600410461\n",
      "Epoch 700/750, Training Loss: 0.8454517126083374, Validation Loss: 0.8040444254875183\n",
      "Epoch 701/750, Training Loss: 0.8389226198196411, Validation Loss: 0.8040263652801514\n",
      "Epoch 702/750, Training Loss: 0.8322715163230896, Validation Loss: 0.803968608379364\n",
      "Epoch 703/750, Training Loss: 0.8372461199760437, Validation Loss: 0.8039249181747437\n",
      "Epoch 704/750, Training Loss: 0.8367006182670593, Validation Loss: 0.8039031624794006\n",
      "Epoch 705/750, Training Loss: 0.838413655757904, Validation Loss: 0.8038501739501953\n",
      "Epoch 706/750, Training Loss: 0.839769184589386, Validation Loss: 0.8037673234939575\n",
      "Epoch 707/750, Training Loss: 0.8365017771720886, Validation Loss: 0.8036693930625916\n",
      "Epoch 708/750, Training Loss: 0.8356120586395264, Validation Loss: 0.803557276725769\n",
      "Epoch 709/750, Training Loss: 0.8352856040000916, Validation Loss: 0.8034427762031555\n",
      "Epoch 710/750, Training Loss: 0.8381162285804749, Validation Loss: 0.8033493161201477\n",
      "Epoch 711/750, Training Loss: 0.8340460658073425, Validation Loss: 0.8032619953155518\n",
      "Epoch 712/750, Training Loss: 0.8415317535400391, Validation Loss: 0.8031786680221558\n",
      "Epoch 713/750, Training Loss: 0.8392312526702881, Validation Loss: 0.8031585216522217\n",
      "Epoch 714/750, Training Loss: 0.833977460861206, Validation Loss: 0.8031321167945862\n",
      "Epoch 715/750, Training Loss: 0.8389557600021362, Validation Loss: 0.8031744956970215\n",
      "Epoch 716/750, Training Loss: 0.8415148258209229, Validation Loss: 0.8032263517379761\n",
      "Epoch 717/750, Training Loss: 0.8350638747215271, Validation Loss: 0.803294837474823\n",
      "Epoch 718/750, Training Loss: 0.832156240940094, Validation Loss: 0.8033978343009949\n",
      "Epoch 719/750, Training Loss: 0.8344588279724121, Validation Loss: 0.8034684062004089\n",
      "Epoch 720/750, Training Loss: 0.8336156606674194, Validation Loss: 0.8035298585891724\n",
      "Epoch 721/750, Training Loss: 0.8284103274345398, Validation Loss: 0.8035317063331604\n",
      "Epoch 722/750, Training Loss: 0.8330397009849548, Validation Loss: 0.803565502166748\n",
      "Epoch 723/750, Training Loss: 0.8312796950340271, Validation Loss: 0.8035331964492798\n",
      "Epoch 724/750, Training Loss: 0.8297404050827026, Validation Loss: 0.8034595847129822\n",
      "Epoch 725/750, Training Loss: 0.8294505476951599, Validation Loss: 0.8033449649810791\n",
      "Epoch 726/750, Training Loss: 0.8354784846305847, Validation Loss: 0.8032172322273254\n",
      "Epoch 727/750, Training Loss: 0.8315092921257019, Validation Loss: 0.8030466437339783\n",
      "Epoch 728/750, Training Loss: 0.8347432613372803, Validation Loss: 0.8028902411460876\n",
      "Epoch 729/750, Training Loss: 0.8280313611030579, Validation Loss: 0.8027728199958801\n",
      "Epoch 730/750, Training Loss: 0.8332220911979675, Validation Loss: 0.8026918768882751\n",
      "Epoch 731/750, Training Loss: 0.8321436047554016, Validation Loss: 0.802649974822998\n",
      "Epoch 732/750, Training Loss: 0.8289289474487305, Validation Loss: 0.8026022911071777\n",
      "Epoch 733/750, Training Loss: 0.827090322971344, Validation Loss: 0.8025808334350586\n",
      "Epoch 734/750, Training Loss: 0.8325982689857483, Validation Loss: 0.8025779128074646\n",
      "Epoch 735/750, Training Loss: 0.8283893465995789, Validation Loss: 0.8025859594345093\n",
      "Epoch 736/750, Training Loss: 0.8272112607955933, Validation Loss: 0.8025992512702942\n",
      "Epoch 737/750, Training Loss: 0.8350438475608826, Validation Loss: 0.8025997877120972\n",
      "Epoch 738/750, Training Loss: 0.8298131227493286, Validation Loss: 0.8025949597358704\n",
      "Epoch 739/750, Training Loss: 0.8250183463096619, Validation Loss: 0.8026219606399536\n",
      "Epoch 740/750, Training Loss: 0.8279626965522766, Validation Loss: 0.8026117086410522\n",
      "Epoch 741/750, Training Loss: 0.8274509906768799, Validation Loss: 0.8026091456413269\n",
      "Epoch 742/750, Training Loss: 0.8236117362976074, Validation Loss: 0.8025974035263062\n",
      "Epoch 743/750, Training Loss: 0.8272729516029358, Validation Loss: 0.8025559782981873\n",
      "Epoch 744/750, Training Loss: 0.8282827138900757, Validation Loss: 0.8025029897689819\n",
      "Epoch 745/750, Training Loss: 0.829622209072113, Validation Loss: 0.8024223446846008\n",
      "Epoch 746/750, Training Loss: 0.82234126329422, Validation Loss: 0.8023457527160645\n",
      "Epoch 747/750, Training Loss: 0.8297896385192871, Validation Loss: 0.8022745847702026\n",
      "Epoch 748/750, Training Loss: 0.8241345286369324, Validation Loss: 0.8022463321685791\n",
      "Epoch 749/750, Training Loss: 0.8267148733139038, Validation Loss: 0.8022008538246155\n",
      "Epoch 750/750, Training Loss: 0.8200860619544983, Validation Loss: 0.802150547504425\n",
      "Validation RMSE: 0.8956279754638672\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 750\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "# 7. Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(X_val_tensor)\n",
    "    val_predictions_clipped = torch.clamp(val_predictions, 0.5, 5.0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_tensor.cpu(), val_predictions_clipped.cpu()))\n",
    "    print(f\"Validation RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global average rating\n",
    "global_avg_rating = train['rating'].mean()\n",
    "\n",
    "# User-specific average rating\n",
    "user_avg_ratings = train.groupby('userId')['rating'].mean().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensor\n",
    "X_test = test[['user', 'movie'] + mlb.classes_.tolist()]\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20168, 22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    # Clip the predictions between 0.5 and 5.0\n",
    "    test_predictions_clipped = torch.clamp(test_predictions, 0.5, 5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to dataframe and save to csv for submission\n",
    "submission_df = pd.DataFrame({'Id': test['Id'], 'rating': test_predictions_clipped.cpu().numpy()})\n",
    "submission_df.to_csv('submission_custom_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
